{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PS2, Part 2: POS Tagging with an LSTM\n",
        "\n",
        "This is adapted from the [PyTorch tutorial on LSTMs for sequence tagging](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html). We are using the same testing data that you used for your HMM POS tagger, and we are using the same training data from which the provided emission probabilities and transition probabilities were dervied.\n",
        "\n",
        "You'll be able to experiment with adjusting the dimensions of embeddings and the hidden layers, as well as the number of epochs, to see whether you can beat the HMM baseline.\n",
        "\n",
        "**In your PDF containing the answers to the HMM questions for Part 1, please include the answers to the final questions, Q6 and Q7, at the very end of this notebook. I have included a reminder about these questions in the README so you don't forget, but you'll need to actually do the work here to answer the question.**\n",
        "\n",
        "Let's get started with importing the libraries we need and mounting your Google Drive.\n"
      ],
      "metadata": {
        "id": "Bw8vocsBP_YO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "152bEP21ZUuy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Prepare the training data\n",
        "\n",
        "First, we need to read in the training data. You did not actually deal with the training data in the Viterbi code because I calculated the probabilities for you.\n",
        "\n",
        "You'll need to mount your Google Drive (as seen in lab 7 and lab 8) and upload `train_pos.txt`, `train_tok.txt`, `test_pos.txt`, and `test_tok.txt` to a folder on your Google Drive called `ps2` for the code below to work. \n",
        "\n",
        "If you'd rather use the little file system thingy in the left panel in Colab, that's fine, but you'll need to rewrite the code below."
      ],
      "metadata": {
        "id": "6TdaWaTuR1VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"/content/drive/MyDrive/ps2/train_pos.txt\")\n",
        "posbyline = f.read().split(\"\\n\")\n",
        "f.close()\n",
        "\n",
        "\n",
        "f = open(\"/content/drive/MyDrive/ps2/train_tok.txt\")\n",
        "tokbyline = f.read().split(\"\\n\")\n",
        "f.close()\n",
        "\n",
        "# just print out the tags and text for a random example so you can see them\n",
        "print(posbyline[2])\n",
        "print(tokbyline[2])"
      ],
      "metadata": {
        "id": "fKwvB6AGaEap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of tuples of lists to store each pair of \n",
        "# token sequence and tag sequence\n",
        "\n",
        "training_data = []\n",
        "for t, p in zip(tokbyline, posbyline):\n",
        "  training_data.append( (t.split(),p.split()))\n",
        "\n",
        "# training_data will be a list of tuples\n",
        "# each tuple will consist of a list of tokens and a list\n",
        "# of their corresponding tags\n",
        "print(training_data[2][0])\n",
        "print(training_data[2][1])"
      ],
      "metadata": {
        "id": "n_trrEGdky7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# My code for reading in the data can't seem to get rid of the final empty line\n",
        "# that gets added to files sometimes, so I'm just deleting it here manually.\n",
        "print(training_data[-1])\n",
        "del(training_data[-1])\n",
        "print(training_data[-1])"
      ],
      "metadata": {
        "id": "9Vp7Eo7Mk_B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code creates a dictionary mapping each word to a unique integer ID,\n",
        "# and each tag to a unique integer ID.\n",
        "# I also create a list of postags for easy lookup by index later on.\n",
        "\n",
        "word_to_ix = {}\n",
        "tag_to_ix = {}\n",
        "postaglist = []\n",
        "\n",
        "# For each tok list and pos list in each tuple of training_data\n",
        "for sent, tags in training_data:\n",
        "\n",
        "    # add any new word to the word dictionary with the next integer ID\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix: \n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "    # add any new tag to the tag dictionary with the next integer ID\n",
        "    for t in tags:\n",
        "        if t not in tag_to_ix:\n",
        "            tag_to_ix[t] = len(tag_to_ix)\n",
        "            postaglist.append(t)\n",
        "\n",
        "# Add unknown word \"UNK\" to word_to_ix.\n",
        "# Doing this in case you find an unknown word in testing.\n",
        "word_to_ix[\"UNK\"] = len(word_to_ix)\n"
      ],
      "metadata": {
        "id": "hm7KE5M_lAuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This little function converts a list of words into a list of their integer IDs\n",
        "# or a list of tags into a list of their integer IDs.\n",
        "# We account for possibility of OOV words by getting the \"UNK\" ID.\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] if w in to_ix else to_ix[\"UNK\"] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)"
      ],
      "metadata": {
        "id": "JMjSLvvzZ3az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Setting up the model\n",
        "\n",
        "For fun, I am setting the embedding dimensions and the hidden dimensions for the model at 8, and the number of epochs at 3. Those are all stupidly small, but it will be interesting to see how well the model does with such a small network and so few epochs. Later on you will change these dimensions to more ambitious values (like 32 or 64 or 128), as well as increasing the number of epochs, to see if you can improve accuracy,"
      ],
      "metadata": {
        "id": "biRgSVzvUU4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 8\n",
        "HIDDEN_DIM = 8\n",
        "EPOCHS = 3"
      ],
      "metadata": {
        "id": "7C7XXx5IlQsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we set up the LSTM itself. When we instantiate it, we'll pass in the two variables above, along with the size of the vocabulary (i.e., the length of `word_to_ix`), and the size of the tagset (i.e., the length of `tag_to_ix`).\n",
        "\n",
        "First we want the model to create embeddings (of the dimension specified by the `embedding_dim` parameter) for the set of possible input tokens (size = `vocab_size`)\n",
        "\n",
        "Then, we have the LSTM layer, which goes from embeddings to a layer of hidden states, with the dimention specified by the `hidden_dim` parameter.\n",
        "\n",
        "Finally, we have a linear layer that maps from the hidden states to the output, i.e., a probability distribution over the possible tags for the input token, whose size specified by the `tagset_size` parameter.\n",
        "\n"
      ],
      "metadata": {
        "id": "VLHHqUS8Utd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "metadata": {
        "id": "_wxHN20dl2AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now instantiate the model,  specify the loss function (here we choose  negative log likelihood), and specify the optimizer (we choose SGD) with its learning rate.*italicized text*"
      ],
      "metadata": {
        "id": "yzwOsoWkWwHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "6Ny7wpW3l-fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Train the model\n",
        "\n",
        "First let's look at the output of the first example before training. Remember, the output will be a probability distribution over the set of possible POS tags. Without training these probabilities will be random. Remember that these are log probabilities so the closer they are to 0, the more probable they are."
      ],
      "metadata": {
        "id": "Qv0q-WKqXXrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For fun, let's see what the probabilities are before we even train.\n",
        "# Element i,j of the output is the score for tag j for word i.\n",
        "# Scores are log probabilities. The closer they are to 0, the more probable.\n",
        "\n",
        "# We don't want to do any training yet,\n",
        "# so the code is wrapped in torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "    print(tag_scores)\n"
      ],
      "metadata": {
        "id": "MKDb4A5VXAyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, now let's train the model as we have parameterized it above."
      ],
      "metadata": {
        "id": "7PW5l2vucJS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# And here the training begins\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    print(f\"Epoch  {epoch}\")\n",
        "\n",
        "    for sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "        # Tensors of word indices using the little function we wrote above.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = prepare_sequence(tags, tag_to_ix)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        tag_scores = model(sentence_in)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        #  calling optimizer.step()\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "print(\"TRAINING COMPLETE\")"
      ],
      "metadata": {
        "id": "6huyvGh0cO1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Evaluate the LSTM on the test data\n",
        "\n",
        "First, read in the test data. It's just like what we did before for reading in the training data."
      ],
      "metadata": {
        "id": "_66WvqzYYM4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Read in test data, just as we read in the training data.\n",
        "f = open(\"/content/drive/MyDrive/ps2/test_pos.txt\")\n",
        "test_posbyline = f.read().split(\"\\n\")\n",
        "print(test_posbyline[2])\n",
        "f.close()\n",
        "\n",
        "f = open(\"/content/drive/MyDrive/ps2/test_tok.txt\")\n",
        "test_tokbyline = f.read().split(\"\\n\")\n",
        "print(test_tokbyline[2])\n",
        "f.close()\n",
        "\n",
        "test_data = []\n",
        "for t, p in zip(test_tokbyline, test_posbyline):\n",
        "  test_data.append( (t.split(),p.split()))\n"
      ],
      "metadata": {
        "id": "Ea8rLqCCCZR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now evaluate! For each test token sequence, convert it to integer IDs, then get the predicted tag sequence. Then compare that tag sequence to the known tag sequence, and count how many tags you get right."
      ],
      "metadata": {
        "id": "WPL1x5YAYs9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No need to train. We're just passing our test data through the\n",
        "# model we trained above, so wrap it in torch.no_grad() again.\n",
        "with torch.no_grad():\n",
        " \n",
        "  # some variable to store how many tags and how many correct\n",
        "  totaltags = 0\n",
        "  totalcorrect = 0\n",
        "\n",
        "  # For each input in the test data and its correct tag sequence...\n",
        "  for toks, tags in test_data:\n",
        "\n",
        "    # Convert the input tokens to integer IDs.\n",
        "    inputs = prepare_sequence(toks, word_to_ix)\n",
        "\n",
        "    # Run that sequence through the model to get the scores (probabilities)\n",
        "    # for each tag in the set of possible tags.\n",
        "    tag_scores = model(inputs)\n",
        "\n",
        "    # Get the predicted tags as follows:\n",
        "    # for each output tensor, find the largest score,\n",
        "    # then look up the tag associated with that index \n",
        "    pred_tag = []\n",
        "    for sc in tag_scores:\n",
        "      pred_tag.append(postaglist[np.argmax(sc)])\n",
        "\n",
        "    # Count up how many of the predicted tags were correct.\n",
        "    for i in range(len(pred_tag)):\n",
        "      totaltags += 1\n",
        "      if pred_tag[i] == tags[i]:\n",
        "        totalcorrect += 1\n",
        "\n",
        "# Print out accuracy\n",
        "print(f\"The accuracy of the model with \\n* {EMBEDDING_DIM}-dimensional embeddings \\\n",
        "and \\n* {HIDDEN_DIM}-dimensional hidden layer \\\n",
        "\\n* trained for {EPOCHS} epochs \\n is {totalcorrect/totaltags}\")"
      ],
      "metadata": {
        "id": "k-D1OF91Ff5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6: Impact of adjusting the hyperparameters\n",
        "Adjust and experiment with the `EMBEDDING_DIM`, `HIDDEN_DIM`, and `EPOCHS` hyperparameters, above. Specifically, you'll want to increase them gradually and see whether even just small changes can result in improvement. Dimensions are usually powers of 2, so you'll jump from 8 to 16 to 32, etc. You won't need to go bigger than 32 or 64. The number of epochs you won't need to go higher than 10.\n",
        "\n",
        "In the PDF with your answers to other questions **create a table** that shows the POS tagging accuracy for each `EMBEDDING_DIM`, `HIDDEN_DIM`, and `EPOCHS` combination that you explore, including the default one provided. Try at least 4 different combinations. Some ideas: increase the layer dimensions but keep epochs small, incease epochs but keep layer dimensions small, increase one layer dimension but not the other, increase everthing a little, increase everything a lot. **The final two rows of the table should be the results you got in part 1 for the most frequent POS tag and the Viterbi search.**\n",
        "\n",
        "# Q7: Which adjustment resulted in the largest improvement? Speculate about why.\n",
        "There is no right or wrong answer. Try to reason about this yourself based on what you have learned in class. The important thing is to show me that you have given this some thought.\n",
        "\n",
        "\n",
        "**Put your table (Q6) and your discussion (Q7) in the PDF you submit with your answers to the other questions from Part 1.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "03M5Gsrwk3bP"
      }
    }
  ]
}